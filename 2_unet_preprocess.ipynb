{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYqoga7HbT8azsQSuco/53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ariegever/ImageProcessing_Project/blob/main/2_unet_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === HOW TO USE THIS NOTEBOOK ===\n",
        "#\n",
        "# 1.  **Check Your Assets:**\n",
        "#     * Go to your Earth Engine Assets tab.\n",
        "#     * You must have 3 assets uploaded:\n",
        "#         1. Your Sentinel-2 Image\n",
        "#         2. Your Sentinel-1 Image\n",
        "#         3. Your Land Cover Mask\n",
        "#     * Get the Asset ID for each one (e.g., \"projects/...\")\n",
        "#\n",
        "# 2.  **Configuration:**\n",
        "#     * Configuration is now handled in `config.py`.\n",
        "#\n",
        "# 3.  **Run All Cells (in order):**\n",
        "#     * The script will save the final `.tfrecord.gz` file to the\n",
        "#       Google Drive path specified in `config.py`."
      ],
      "metadata": {
        "id": "alKoxk6ughxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kfDiOAO10vE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import config\n",
        "drive.mount(config.DRIVE_MOUNT_PATH)\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "import ee\n",
        "# Trigger the authentication flow.\n",
        "auth.authenticate_user()\n",
        "# Get credentials and initialize Earth Engine\n",
        "credentials, project = google.auth.default()\n",
        "ee.Initialize(credentials, project=config.PROJECT_ID, opt_url='https://earthengine-highvolume.googleapis.com')\n",
        "\n",
        "print(f\"Successfully initialized Earth Engine for project: {config.PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import requests\n",
        "import concurrent.futures\n",
        "from google.api_core import retry\n",
        "from numpy.lib import recfunctions as rfn\n",
        "from PIL import ImageColor\n",
        "from matplotlib.colors import ListedColormap\n",
        "from skimage.exposure import rescale_intensity\n",
        "import utils"
      ],
      "metadata": {
        "id": "vlKoCp9m-_XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Configuration from config.py ===\n",
        "\n",
        "# Create the output directory in Google Drive\n",
        "os.makedirs(config.DRIVE_IMAGES_PATH, exist_ok=True)\n",
        "OUTPUT_FILE = os.path.join(config.DRIVE_IMAGES_PATH, config.TFRECORD_FILE)\n",
        "\n",
        "print(f\"Project: {config.PROJECT_ID}\")\n",
        "print(f\"Feature bands: {config.FEATURE_NAMES}\")\n",
        "print(f\"Output will be saved to: {OUTPUT_FILE}\")\n",
        "print(f\" expecting coordinates at: {config.POINTS_ASSET_PATH}\")"
      ],
      "metadata": {
        "id": "9Lxngtb__Agf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "try:\n",
        "    with open(config.CLASS_JSON_PATH) as f:\n",
        "        lc = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: '{config.CLASS_JSON_PATH}' not found.\")\n",
        "    raise\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"ERROR: '{config.CLASS_JSON_PATH}' is not a valid JSON file.\")\n",
        "    raise\n",
        "\n",
        "# Load from a dict (object) instead of a list\n",
        "lc_df = pd.DataFrame.from_dict(lc, orient='index')\n",
        "\n",
        "# Rename your columns to match what the script expects\n",
        "lc_df = lc_df.rename(columns={'class': 'label', 'color': 'palette'})\n",
        "\n",
        "# This line IS THE SAME AS YOUR SCRIPT. It creates new normalized values 1, 2, 3, 4, 5\n",
        "lc_df[\"values_normalize\"] = lc_df.index.astype(int) + 1\n",
        "\n",
        "from_values = []\n",
        "to_values = []\n",
        "\n",
        "for index, row in lc_df.iterrows():\n",
        "    # Get the new value this script just created (1, 2, 3, 4, or 5)\n",
        "    new_normalized_value = row['values_normalize']\n",
        "\n",
        "    # Find all original values that map to it\n",
        "    for original_class in row['original_classes']:\n",
        "        from_values.append(original_class['values'])\n",
        "        to_values.append(new_normalized_value)\n",
        "\n",
        "# Get palette for plotting\n",
        "palette_hex = lc_df[\"palette\"].to_list()\n",
        "\n",
        "cmap = ListedColormap(palette_hex)\n",
        "vmin = 1\n",
        "vmax = len(palette_hex) # This will be 5\n",
        "\n",
        "print(f\"Loaded {len(lc_df)} AGGREGATED classes from {config.CLASS_JSON_PATH}.\")\n",
        "print(f\"Remapping {from_values} -> {to_values}\")\n",
        "lc_df\n",
        "\n"
      ],
      "metadata": {
        "id": "CMuwigSqBNA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the S2, S1, and Land Cover images\n",
        "try:\n",
        "    s2_image = ee.Image(config.S2_ASSET_ID).select(config.S2_BANDS)\n",
        "    s1_image = ee.Image(config.S1_ASSET_ID).select(config.S1_BANDS)\n",
        "    lc_image = ee.Image(config.LC_ASSET_ID) # This should be a single-band image\n",
        "except ee.EEException as e:\n",
        "    print(f\"Error loading assets: {e}\")\n",
        "    print(\"Check your Asset Paths in config.py.\")\n",
        "    raise\n",
        "\n",
        "# 2. Combine S1 and S2 into one feature image\n",
        "feature_image = s2_image.addBands(s1_image)\n",
        "\n",
        "# 3. Remap the land cover image\n",
        "lc_remapped = lc_image.remap(from_values, to_values, 0, lc_image.bandNames().get(0))\n",
        "lc_remapped = lc_remapped.rename(config.LABEL_NAME).toUint8() # Rename band to 'label'\n",
        "\n",
        "# 4. Stack all bands into one image (features + label)\n",
        "all_bands_image = feature_image.addBands(lc_remapped)\n",
        "\n",
        "ALL_BANDS = config.FEATURE_NAMES + [config.LABEL_NAME]\n",
        "print(\"Earth Engine assets loaded and stacked.\")\n",
        "print(f\"Total bands in stacked image: {ALL_BANDS}\")"
      ],
      "metadata": {
        "id": "w5ZfihWSC_Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch extraction functions are now in utils.py\n",
        "print(\"Patch extraction functions defined in utils.py.\")"
      ],
      "metadata": {
        "id": "zCmA7_Q2DNsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coordinate loading function is now in utils.py\n",
        "print(\"Coordinate loading function defined in utils.py.\")"
      ],
      "metadata": {
        "id": "ryNqoujlIGCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def array_to_example(structured_array):\n",
        "  \"\"\"Serialize a structured numpy array into a tf.Example proto.\"\"\"\n",
        "  feature = {}\n",
        "\n",
        "  # Create default \"empty\" patches of all zeros\n",
        "  default_float_patch = np.zeros(config.PATCH_SIZE * config.PATCH_SIZE, dtype=np.float32)\n",
        "  default_int_patch = np.zeros(config.PATCH_SIZE * config.PATCH_SIZE, dtype=np.int64)\n",
        "\n",
        "  # Loop through all expected FEATURE bands\n",
        "  for f in config.FEATURE_NAMES:\n",
        "    if (f in structured_array.dtype.names and\n",
        "        structured_array[f].size == config.PATCH_SIZE * config.PATCH_SIZE):\n",
        "\n",
        "        patch_data = structured_array[f].flatten().astype(np.float32)\n",
        "        # Handle NaNs if any (though GEE usually handles this, good to be safe)\n",
        "        patch_data = np.nan_to_num(patch_data, nan=0.0)\n",
        "        feature[f] = tf.train.Feature(float_list=tf.train.FloatList(value=patch_data))\n",
        "    else:\n",
        "        # Fallback for missing bands (shouldn't happen if assets are correct)\n",
        "        feature[f] = tf.train.Feature(float_list=tf.train.FloatList(value=default_float_patch))\n",
        "\n",
        "  # Handle the LABEL band\n",
        "  if (config.LABEL_NAME in structured_array.dtype.names and\n",
        "      structured_array[config.LABEL_NAME].size == config.PATCH_SIZE * config.PATCH_SIZE):\n",
        "\n",
        "      label_data = structured_array[config.LABEL_NAME].flatten().astype(np.int64)\n",
        "      feature[config.LABEL_NAME] = tf.train.Feature(int64_list=tf.train.Int64List(value=label_data))\n",
        "  else:\n",
        "      feature[config.LABEL_NAME] = tf.train.Feature(int64_list=tf.train.Int64List(value=default_int_patch))\n",
        "\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))"
      ],
      "metadata": {
        "id": "array_to_example"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# 1. Load coordinates\n",
        "coords_list = utils.load_coords_from_asset(config.POINTS_ASSET_PATH)\n",
        "\n",
        "if not coords_list:\n",
        "    print(\"No coordinates found. Exiting.\")\n",
        "else:\n",
        "    print(f\"Found {len(coords_list)} points. Starting patch extraction...\")\n",
        "\n",
        "    # 2. Open TFRecord writer\n",
        "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "    with tf.io.TFRecordWriter(OUTPUT_FILE, options=options) as writer:\n",
        "        success_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        # 3. Process points (Sequential for simplicity and to avoid rate limits)\n",
        "        # You can use concurrent.futures for parallel processing if you handle rate limits.\n",
        "        for i, coords in enumerate(coords_list):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processing point {i+1}/{len(coords_list)}...\")\n",
        "\n",
        "            try:\n",
        "                # Fetch patch\n",
        "                patch_array = utils.get_patch(coords, all_bands_image)\n",
        "\n",
        "                # Serialize\n",
        "                example = array_to_example(patch_array)\n",
        "\n",
        "                # Write\n",
        "                writer.write(example.SerializeToString())\n",
        "                success_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing point {i}: {e}\")\n",
        "                error_count += 1\n",
        "\n",
        "    print(f\"\\nProcessing complete.\")\n",
        "    print(f\"Successfully wrote {success_count} patches.\")\n",
        "    print(f\"Failed points: {error_count}\")\n",
        "    print(f\"Output file: {OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "main_execution"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}